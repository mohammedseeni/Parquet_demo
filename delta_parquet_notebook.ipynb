{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing pyarrow and pyspark\n",
    "# pip install pyarrow\n",
    "# pip install delta-spark==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.version)\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"TestApp\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(f\"spark version : {spark.version}\")\n",
    "# spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Initialize Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"id\"])\n",
    "\n",
    "# Test writing to a Delta table\n",
    "path = \"/tmp/delta-table\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "# Test reading from a Delta table\n",
    "df_read = spark.read.format(\"delta\").load(path)\n",
    "df_read.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating a Sample CSV File\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a DataFrame\n",
    "data = {\n",
    "    \"id\": np.arange(1, 11),\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Fiona\", \"Gina\", \"Harry\", \"Ivy\", \"John\"],\n",
    "    \"age\": np.random.randint(18, 40, size=10)\n",
    "}\n",
    "\n",
    "# write the content of df to CSV file\n",
    "df = pd.DataFrame(data)\n",
    "print(\"df to csv\")\n",
    "print(df)\n",
    "\n",
    "# Saving to CSV\n",
    "csv_file_path = 'sample_data.csv'\n",
    "print(csv_file_path)\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(\"\\n after csv file creation\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Converting CSV to Parquet\n",
    "\n",
    "# Reading CSV\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Converting to Parquet\n",
    "parquet_file_path = 'sample_data.parquet'\n",
    "df.to_parquet(parquet_file_path, index=False)\n",
    "print(\"after converting csv file to parquet file\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Accessing Parquet Metadata and Viewing Blocks\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Opening the Parquet file\n",
    "parquet_file = pq.ParquetFile(parquet_file_path)\n",
    "\n",
    "# Viewing Metadata\n",
    "print(parquet_file.metadata)\n",
    "print(parquet_file.schema)\n",
    "\n",
    "# Viewing the data by row group and column\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    rg = parquet_file.read_row_group(i)\n",
    "    print(f\"Row Group {i}:\")\n",
    "    print(rg.to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Converting CSV to Delta-Parquet and Viewing Data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Initialize Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ensure the DataFrame from the CSV is loaded into Spark\n",
    "df_spark = spark.read.csv('sample_data.csv', header=True, inferSchema=True)\n",
    "print(df_spark)\n",
    "\n",
    "# Define a path where the Delta table will be stored\n",
    "path_to_delta_table = \"/tmp/delta-table\"\n",
    "\n",
    "# Write DataFrame to Delta format\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").save(path_to_delta_table)\n",
    "\n",
    "# Reading from the Delta table\n",
    "delta_df = spark.read.format(\"delta\").load(path_to_delta_table)\n",
    "delta_df.show()\n",
    "\n",
    "# Accessing DeltaTable functions\n",
    "delta_table = DeltaTable.forPath(spark, path_to_delta_table)\n",
    "full_history = delta_table.history()  # Provides a DataFrame with the full history of the table\n",
    "full_history.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Delta support\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"DeltaExample\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "#     .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "test_df = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2)], [\"name\", \"id\"])\n",
    "\n",
    "\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
