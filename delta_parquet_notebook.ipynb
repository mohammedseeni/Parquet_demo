{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing pyarrow and pyspark\n",
    "# pip install pyarrow\n",
    "# pip install pyspark delta-spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df to csv\n",
      "   id     name  age\n",
      "0   1    Alice   28\n",
      "1   2      Bob   33\n",
      "2   3  Charlie   24\n",
      "3   4    David   31\n",
      "4   5      Eva   39\n",
      "5   6    Fiona   20\n",
      "6   7     Gina   26\n",
      "7   8    Harry   22\n",
      "8   9      Ivy   29\n",
      "9  10     John   34\n",
      "\n",
      " after csv file creation\n",
      "   id     name  age\n",
      "0   1    Alice   28\n",
      "1   2      Bob   33\n",
      "2   3  Charlie   24\n",
      "3   4    David   31\n",
      "4   5      Eva   39\n",
      "5   6    Fiona   20\n",
      "6   7     Gina   26\n",
      "7   8    Harry   22\n",
      "8   9      Ivy   29\n",
      "9  10     John   34\n"
     ]
    }
   ],
   "source": [
    "# 1. Creating a Sample CSV File\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a DataFrame\n",
    "data = {\n",
    "    \"id\": np.arange(1, 11),\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Fiona\", \"Gina\", \"Harry\", \"Ivy\", \"John\"],\n",
    "    \"age\": np.random.randint(18, 40, size=10)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"df to csv\")\n",
    "print(df)\n",
    "\n",
    "# Saving to CSV\n",
    "csv_file_path = 'sample_data.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "print(\"\\n after csv file creation\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after converting csv file to parquet file\n",
      "   id     name  age\n",
      "0   1    Alice   25\n",
      "1   2      Bob   34\n",
      "2   3  Charlie   34\n",
      "3   4    David   26\n",
      "4   5      Eva   38\n",
      "5   6    Fiona   34\n",
      "6   7     Gina   27\n",
      "7   8    Harry   24\n",
      "8   9      Ivy   21\n",
      "9  10     John   21\n"
     ]
    }
   ],
   "source": [
    "# 2. Converting CSV to Parquet\n",
    "\n",
    "# Reading CSV\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Converting to Parquet\n",
    "parquet_file_path = 'sample_data.parquet'\n",
    "df.to_parquet(parquet_file_path, index=False)\n",
    "print(\"after converting csv file to parquet file\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Accessing Parquet Metadata and Viewing Blocks\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Opening the Parquet file\n",
    "parquet_file = pq.ParquetFile(parquet_file_path)\n",
    "\n",
    "# Viewing Metadata\n",
    "print(parquet_file.metadata)\n",
    "print(parquet_file.schema)\n",
    "\n",
    "# Viewing the data by row group and column\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    rg = parquet_file.read_row_group(i)\n",
    "    print(f\"Row Group {i}:\")\n",
    "    print(rg.to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 17:31:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 34|\n",
      "|  3|Charlie| 34|\n",
      "|  4|  David| 26|\n",
      "|  5|    Eva| 38|\n",
      "|  6|  Fiona| 34|\n",
      "|  7|   Gina| 27|\n",
      "|  8|  Harry| 24|\n",
      "|  9|    Ivy| 21|\n",
      "| 10|   John| 21|\n",
      "+---+-------+---+\n",
      "\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2024-05-08 17:31:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Converting CSV to Delta-Parquet and Viewing Data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Initialize Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ensure the DataFrame from the CSV is loaded into Spark\n",
    "df_spark = spark.read.csv('sample_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Define a path where the Delta table will be stored\n",
    "# Make sure to change \"/tmp/delta-table\" to a suitable path where you have write permissions\n",
    "path_to_delta_table = \"/tmp/delta-table\"\n",
    "\n",
    "# Write DataFrame to Delta format\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").save(path_to_delta_table)\n",
    "\n",
    "# Reading from the Delta table\n",
    "delta_df = spark.read.format(\"delta\").load(path_to_delta_table)\n",
    "delta_df.show()\n",
    "\n",
    "# Accessing DeltaTable functions\n",
    "delta_table = DeltaTable.forPath(spark, path_to_delta_table)\n",
    "full_history = delta_table.history()  # Provides a DataFrame with the full history of the table\n",
    "full_history.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
